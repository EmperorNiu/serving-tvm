syntax = "proto3";

package tensorflow.serving;

// Configuration parameters for a TVM, with optional batching.
message TVMConfig {
  string session_target = 1;
  //BatchingParameters batching_parameters = 2;
}
/*
// Batching parameters. Each individual parameter is optional. If omitted, the
// default value from the relevant batching config struct (SharedBatchScheduler
// ::Options or BatchSchedulerRetrier::Options) is used.
message BatchingParameters {
  // SharedBatchScheduler options (see shared_batch_scheduler.h):
  //

  // The maximum size of each batch.
  //
  // IMPORTANT: As discussed above, use 'max_batch_size * 2' client threads to
  // achieve high throughput with batching.
  //google.protobuf.Int64Value max_batch_size = 1; - TODO

  // If a task has been enqueued for this amount of time (in microseconds), and
  // a thread is available, the scheduler will immediately form a batch from
  // enqueued tasks and assign the batch to the thread for processing, even if
  // the batch's size is below 'max_batch_size'.
  //google.protobuf.Int64Value batch_timeout_micros = 2; - TODO

  // The maximum length of the queue, in terms of the number of batches. (A
  // batch that has been scheduled on a thread is considered to have been
  // removed from the queue.)
  //google.protobuf.Int64Value max_enqueued_batches = 3; - TODO

  // The number of threads to use to process batches.
  // Must be >= 1, and should be tuned carefully.
  // google.protobuf.Int64Value num_batch_threads = 4; - TODO

  // The name to use for the pool of batch threads.
  // google.protobuf.StringValue thread_pool_name = 5; - TODO

  // BatchingSession options (see batching_session.h):
  //

  // The allowed batch sizes. (Ignored if left empty.)
  // Requirements:
  //  - The entries must be in increasing order.
  //  - The final entry must equal 'max_batch_size'.
  repeated int64 allowed_batch_sizes = 1;

  // Whether to pad variable-length inputs when a batch is formed.
  bool pad_variable_length_inputs = 2;
}
*/
